vocab: 'ÄÖßëŠšžŽĆćçÇǦ³°ËÜÛôÔüÈèöäÓóõíÍĂăẢảaAáÁbBcCdDeÉẺEẻéfFgGhHiIjJkKlLmMnNoOpPqQrRsStTuUvVwWxXyYzZ0123456789!"#$%&''()*+,-./:;<=>?@[\]^_`{|}~ '

#pretrain:
#    id_or_url: 13327Y1tz1ohsm5YZMyXVMPIOjoOA0OaA
#    md5: fbefa85079ad9001a71eb1bf47a93785
#    cached: /tmp/tranformerorc.pth

# url or local path
device: cpu

trainer:
    batch_size: 12
    print_every: 200
    valid_every: 4000
    iters: 1000000
    # where to save our model for prediction
    export: ./weights/transformerocr.pth
    export2: ./weights/transformerocr2.pth
    checkpoint: ./checkpoint/transformerocr_checkpoint.pth
    log: ./train.log
    # null to disable compuate accuracy, or change to number of sample to enable validiation while training
    metrics: null

dataset:
    # name of your dataset
    name: data
    # path to annotation and image
    data_root: ./img/
    train_annotation: done.txt
    valid_annotation: val.txt
    # resize image to 32 height, larger height will increase accuracy
    image_height: 32
    image_min_width: 32
    image_max_width: 512

dataloader:
    num_workers: 0
    pin_memory: True

aug:
    image_aug: True
    masked_language_model: False

predictor:
    # disable or enable beamsearch while prediction, use beamsearch will be slower
    beamsearch: False

quiet: False
weights: weight/recognition/transformerocr2.pth

backbone: vgg19_bn
cnn:
    # pooling stride size
    ss:
        - [2, 2]
        - [2, 2]
        - [2, 1]
        - [2, 1]
        - [1, 1]
    # pooling kernel size
    ks:
        - [2, 2]
        - [2, 2]
        - [2, 1]
        - [2, 1]
        - [1, 1]
    # dim of ouput feature map
    hidden: 256

seq_modeling: seq2seq
transformer:
    encoder_hidden: 256
    decoder_hidden: 256
    img_channel: 256
    decoder_embedded: 256
    dropout: 0.1

optimizer:
    max_lr: 0.001
    pct_start: 0.1
